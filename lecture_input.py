import os
import zipfile
import subprocess
import tempfile
import logging
import gradio as gr
import xml.etree.ElementTree as ET
from shutil import which
from gtts import gTTS
from pptx import Presentation
from pdf2image import convert_from_path
from src.utils.math_formula_processor import MathFormulaProcessor, process_math_text
from src.voice.enrollment import enroll_voice
# Th√™m import backend voice
from src.voice.store import list_voices, has_voice
from src.voice.tts_engine import synthesize

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)


def convert_pptx_to_images(pptx_path, dpi=220):
    if which("soffice") is None:
        raise RuntimeError("Kh√¥ng t√¨m th·∫•y LibreOffice (soffice). Vui l√≤ng c√†i LibreOffice ƒë·ªÉ chuy·ªÉn PPTX -> PDF.")

    tmpdir = tempfile.mkdtemp(prefix="pptx2img_")
    try:
        subprocess.run(
            ["soffice", "--headless", "--convert-to", "pdf", "--outdir", tmpdir, pptx_path],
            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        )
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Chuy·ªÉn PPTX sang PDF th·∫•t b·∫°i: {e}")

    pdf_path = os.path.join(tmpdir, os.path.splitext(os.path.basename(pptx_path))[0] + ".pdf")
    if not os.path.exists(pdf_path):
        raise RuntimeError("Kh√¥ng t·∫°o ƒë∆∞·ª£c PDF t·ª´ PPTX. Ki·ªÉm tra file ƒë·∫ßu v√†o.")

    try:
        images = convert_from_path(pdf_path, dpi=dpi, output_folder=tmpdir, fmt='png')
    except Exception as e:
        raise RuntimeError("L·ªói convert PDF -> ·∫£nh. C√≥ th·ªÉ thi·∫øu Poppler (poppler-utils).") from e

    img_paths = []
    for i, img in enumerate(images, 1):
        img_path = os.path.join(tmpdir, f"slide-{i:02d}.png")
        img.save(img_path)
        img_paths.append(img_path)
    return img_paths


def convert_text_to_audio(text, language='vi'):
    try:
        if not text or text.strip() == "":
            return None
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
        temp_path = temp_file.name
        temp_file.close()
        tts = gTTS(text=text, lang=language, slow=False)
        tts.save(temp_path)
        return temp_path
    except Exception:
        return None

def convert_text_to_audio_with_voice(text, user_id, voice_id, language=None):
    """
    Convert text to audio using registered voice (XTTS v2)
    Falls back to gTTS if XTTS v2 is not available
    """
    try:
        if not text or text.strip() == "":
            return None
            
        print(f"üîç Debug: text='{text[:50]}...', user_id='{user_id}', voice_id='{voice_id}'")
            
        # B·∫Øt bu·ªôc ph·∫£i c√≥ voice
        if not has_voice(user_id, voice_id):
            print(f"‚ùå Voice not found: {voice_id}")
            raise Exception(f"Gi·ªçng n√≥i '{voice_id}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ƒëƒÉng k√Ω gi·ªçng n√≥i tr∆∞·ªõc!")
        
        print(f"üé§ Using registered voice: {voice_id}")
        
        # L·∫•y th√¥ng tin voice ƒë·ªÉ bi·∫øt ng√¥n ng·ªØ
        from src.voice.store import load_profile
        emb, meta = load_profile(user_id, voice_id)
        voice_language = meta.get('lang_hint', 'vi')
        model_type = meta.get('model_type', 'unknown')
        
        print(f"üåê Using voice language: {voice_language}")
        print(f"ü§ñ Model type: {model_type}")
        
        # Ki·ªÉm tra n·∫øu model l√† Tacotron2 (kh√¥ng h·ªó tr·ª£ voice cloning)
        if model_type == "tacotron2":
            print(f"‚ö†Ô∏è Voice was registered with Tacotron2 (no voice cloning support)")
            print(f"üîä Falling back to gTTS with voice language: {voice_language}")
            return convert_text_to_audio(text, voice_language)
        
        # S·ª≠ d·ª•ng gi·ªçng n√≥i ƒë√£ ƒëƒÉng k√Ω v·ªõi ng√¥n ng·ªØ c·ªßa voice
        audio_path = synthesize(text, user_id, voice_id, voice_language)
        print(f"üîç synthesize() returned: {audio_path}")
        
        if audio_path and os.path.exists(audio_path):
            print(f"‚úÖ Successfully synthesized with registered voice: {audio_path}")
            return audio_path
        else:
            print(f"‚ùå Failed to synthesize with registered voice")
            print(f"üîç audio_path: {audio_path}")
            print(f"üîç exists: {os.path.exists(audio_path) if audio_path else 'None'}")
            print(f"üîä Falling back to gTTS with voice language: {voice_language}")
            return convert_text_to_audio(text, voice_language)
        
    except Exception as e:
        print(f"‚ùå Error in convert_text_to_audio_with_voice: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Fallback to gTTS with voice language
        try:
            voice_language = 'vi'  # Default fallback
            if has_voice(user_id, voice_id):
                emb, meta = load_profile(user_id, voice_id)
                voice_language = meta.get('lang_hint', 'vi')
            
            print(f"üîä Final fallback to gTTS with language: {voice_language}")
            return convert_text_to_audio(text, voice_language)
        except Exception as fallback_error:
            print(f"‚ùå Even gTTS fallback failed: {fallback_error}")
            raise e


def read_text_file(file):
    if file is None:
        return "", "‚ùå Vui l√≤ng ch·ªçn file vƒÉn b·∫£n!"
    try:
        file_path = file.name
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext not in ['.txt', '.md', '.doc', '.docx', '.rtf']:
            return "", "‚ùå Ch·ªâ h·ªó tr·ª£ file vƒÉn b·∫£n (.txt, .md, .doc, .docx, .rtf)"
        content = ""
        if file_ext == '.docx':
            try:
                with zipfile.ZipFile(file_path, 'r') as zip_file:
                    if 'word/document.xml' in zip_file.namelist():
                        xml_content = zip_file.read('word/document.xml')
                        root = ET.fromstring(xml_content)
                        text_elements = []
                        for elem in root.iter():
                            if elem.text and elem.text.strip():
                                text_elements.append(elem.text.strip())
                        content = ' '.join(text_elements)
                    else:
                        return "", "‚ùå Kh√¥ng th·ªÉ ƒë·ªçc n·ªôi dung t·ª´ file .docx"
            except Exception as e:
                return "", f"‚ùå L·ªói ƒë·ªçc file .docx: {str(e)}"
        elif file_ext == '.doc':
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                try:
                    with open(file_path, 'r', encoding='latin-1') as f:
                        content = f.read()
                except:
                    return "", "‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file .doc. Vui l√≤ng chuy·ªÉn ƒë·ªïi sang .txt ho·∫∑c .docx"
        else:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read()
        content = ''.join(char for char in content if ord(char) >= 32 or char in '\n\r\t').strip()
        if not content:
            return "", "‚ùå File vƒÉn b·∫£n tr·ªëng ho·∫∑c ch·ªâ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát!"
        return content, f"‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file: {os.path.basename(file_path)} ({len(content)} k√Ω t·ª± th·ª±c t·∫ø)"
    except Exception as e:
        return "", f"‚ùå L·ªói ƒë·ªçc file: {str(e)}"


def extract_slides_from_pptx(pptx_file):
    slides_data = []
    image_paths = convert_pptx_to_images(pptx_file.name, dpi=220)
    math_processor = MathFormulaProcessor()
    processed_result = math_processor.process_powerpoint_text(pptx_file.name)

    if processed_result.get('error'):
        logger.warning(f"L·ªói x·ª≠ l√Ω c√¥ng th·ª©c to√°n h·ªçc: {processed_result['error']}, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p c≈©")
        prs = Presentation(pptx_file.name)
        for i, slide in enumerate(prs.slides):
            text_chunks = []
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text:
                    text_chunks.append(shape.text.strip())
            text = " ".join(filter(None, text_chunks))
            processed_text = process_math_text(text.strip())
            slides_data.append({
                'slide_number': i + 1,
                'text': processed_text,
                'image_path': image_paths[i] if i < len(image_paths) else None,
                'has_math_objects': False
            })
    else:
        for slide_info in processed_result['slides']:
            slides_data.append({
                'slide_number': slide_info['slide_number'],
                'text': slide_info['processed_text'],
                'image_path': image_paths[slide_info['slide_number'] - 1] if slide_info['slide_number'] - 1 < len(image_paths) else None,
                'has_math_objects': slide_info['has_math_objects']
            })
    return slides_data


def extract_lecture_slides(file):
    if file:
        slides_data = extract_slides_from_pptx(file)
        if slides_data:
            slides_text = []
            total_math_slides = 0
            for slide in slides_data:
                slide_num = slide['slide_number']
                text_preview = slide['text'][:100] + "..." if len(slide['text']) > 100 else slide['text']
                if slide.get('has_math_objects', False):
                    slides_text.append(f"Slide {slide_num}: {text_preview} [üìê C√≥ c√¥ng th·ª©c to√°n h·ªçc]")
                    total_math_slides += 1
                else:
                    slides_text.append(f"Slide {slide_num}: {text_preview}")
            summary = f"\n\nüìä Th·ªëng k√™: {len(slides_data)} slides, {total_math_slides} slides c√≥ c√¥ng th·ª©c to√°n h·ªçc"
            if total_math_slides > 0:
                summary += "\n‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát v√† c√¥ng th·ª©c to√°n h·ªçc!"
            return "\n\n".join(slides_text) + summary
    return "‚ùå Vui l√≤ng ch·ªçn file PowerPoint!"


def set_lecture_fast_mode():
    return 256, 'crop', False, 6, False, 0

def handle_register_voice(file_obj):
    if file_obj is None:
        return "‚ùå Ch∆∞a upload file", gr.update(choices=[])
    
    audio_path = file_obj.name  # l·∫•y ƒë∆∞·ªùng d·∫´n t·∫°m m√† Gradio l∆∞u file
    voice_id = os.path.basename(audio_path).split('.')[0]
    
    print(f"üîç Debug: ƒêang ƒëƒÉng k√Ω voice '{voice_id}' t·ª´ file: {audio_path}")
    
    # Ki·ªÉm tra voice ƒë√£ t·ªìn t·∫°i ch∆∞a
    if has_voice("current_user", voice_id):
        print(f"‚ö†Ô∏è  Voice '{voice_id}' ƒë√£ t·ªìn t·∫°i, s·∫Ω ghi ƒë√®")
    
    # S·ª≠ d·ª•ng ng√¥n ng·ªØ m·∫∑c ƒë·ªãnh l√† 'vi' cho gi·ªçng nh√¢n b·∫£n
    success = enroll_voice(audio_path, "current_user", voice_id, lang_hint="vi")
    
    if not success:
        print(f"‚ùå ƒêƒÉng k√Ω voice '{voice_id}' th·∫•t b·∫°i")
        return "‚ùå ƒêƒÉng k√Ω gi·ªçng th·∫•t b·∫°i", gr.update(choices=[])
    
    print(f"‚úÖ ƒêƒÉng k√Ω voice '{voice_id}' th√†nh c√¥ng")
    
    # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o file ƒë∆∞·ª£c l∆∞u
    import time
    time.sleep(0.5)
    
    # Refresh dropdown v·ªõi danh s√°ch gi·ªçng m·ªõi
    voices = list_voices("current_user")
    voice_choices = [m["voice_id"] for m in voices]
    
    print(f"üîç Debug: Danh s√°ch voices sau khi ƒëƒÉng k√Ω: {voice_choices}")
    print(f"üîç Debug: Voice ID hi·ªán t·∫°i: {voice_id}")
    print(f"üîç Debug: Voice ID c√≥ trong danh s√°ch: {voice_id in voice_choices}")
    
    # T√¨m voice profile m·ªõi nh·∫•t ƒë·ªÉ l·∫•y th√¥ng tin
    if voices:
        latest_voice = voices[0]  # voices ƒë√£ ƒë∆∞·ª£c sort theo created_at
        duration = latest_voice.get("audio_length", 0) / latest_voice.get("sample_rate", 48000)
        print(f"‚úÖ Voice '{voice_id}' ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi duration: {duration:.1f}s")
        return f"‚úÖ ƒê√£ l∆∞u gi·ªçng '{voice_id}' (~{duration:.1f}s)", gr.update(choices=voice_choices, value=voice_id)
    else:
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y voices sau khi ƒëƒÉng k√Ω")
        return f"‚úÖ ƒê√£ l∆∞u gi·ªçng '{voice_id}'", gr.update(choices=voice_choices, value=voice_id)

def create_lecture_input_interface():
    with gr.Row(equal_height=False):        
        with gr.Column(variant='panel'):
            gr.Markdown("### üë®‚Äçüè´ ·∫¢nh Gi√°o Vi√™n")
            lecture_source_image = gr.Image(label="·∫¢nh khu√¥n m·∫∑t gi√°o vi√™n", source="upload", type="filepath")

            gr.Markdown("### üìä File PowerPoint B√†i Gi·∫£ng")
            lecture_pptx_file = gr.File(label="Ch·ªçn file PowerPoint (.pptx)", file_types=[".pptx"])

            gr.Markdown("### üé§ Gi·ªçng Nh√¢n B·∫£n (B·∫Øt Bu·ªôc)")
            gr.Markdown("*‚ö†Ô∏è B·∫°n ph·∫£i ƒëƒÉng k√Ω gi·ªçng n√≥i tr∆∞·ªõc khi t·∫°o video b√†i gi·∫£ng*")
            
            # Upload audio voice sample
            lecture_voice_sample = gr.File(
                label="üì¢ Upload audio m·∫´u ƒë·ªÉ nh√¢n b·∫£n gi·ªçng",
                file_types=[".wav", ".mp3"],
                type="file"  # thay filepath th√†nh file
            )
            # N√∫t ƒëƒÉng k√Ω gi·ªçng
            register_voice_btn = gr.Button("üíæ ƒêƒÉng k√Ω gi·ªçng", variant="primary")
            register_voice_status = gr.Textbox(
                label="Tr·∫°ng th√°i ƒëƒÉng k√Ω gi·ªçng",
                interactive=False
            )
            
            # Button debug ƒë·ªÉ ki·ªÉm tra voice store
            debug_voice_btn = gr.Button("üîç Debug Voice Store", variant="secondary", size="sm")

            lecture_voice_id = gr.Dropdown(
                choices=[m["voice_id"] for m in list_voices("current_user")],
                label="üé§ Ch·ªçn gi·ªçng nh√¢n b·∫£n (B·∫Øt bu·ªôc)",
                value=None,
                info="Ng√¥n ng·ªØ s·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ gi·ªçng ƒë√£ ƒëƒÉng k√Ω"
            )

            # Khi b·∫•m n√∫t -> g·ªçi h√†m enroll_voice v√† refresh dropdown
            register_voice_btn.click(
                fn=handle_register_voice,
                inputs=[lecture_voice_sample],
                outputs=[register_voice_status, lecture_voice_id]
            )
            
            # Th√™m event ƒë·ªÉ refresh dropdown khi c√≥ thay ƒë·ªïi
            def refresh_voice_dropdown():
                voices = list_voices("current_user")
                voice_choices = [m["voice_id"] for m in voices]
                print(f"üîç Debug: Refreshing voice dropdown, found {len(voices)} voices: {voice_choices}")
                return gr.update(choices=voice_choices)

            def debug_voice_store():
                """Debug function ƒë·ªÉ ki·ªÉm tra voice store"""
                print("üîç Debug: Ki·ªÉm tra voice store...")
                
                # Ki·ªÉm tra th∆∞ m·ª•c voices
                from src.voice.store import ROOT
                import os
                voices_dir = ROOT / "current_user"
                print(f"üîç Voice directory: {voices_dir}")
                print(f"üîç Directory exists: {voices_dir.exists()}")
                
                if voices_dir.exists():
                    for item in voices_dir.iterdir():
                        print(f"üîç Found item: {item.name} (dir: {item.is_dir()})")
                        if item.is_dir():
                            meta_file = item / "meta.json"
                            emb_file = item / "embedding.npy"
                            print(f"  - meta.json exists: {meta_file.exists()}")
                            print(f"  - embedding.npy exists: {emb_file.exists()}")
                
                # Ki·ªÉm tra list_voices function
                voices = list_voices("current_user")
                print(f"üîç list_voices result: {voices}")
                
                return f"Debug complete. Found {len(voices)} voices."
            
            # Button debug voice store
            debug_voice_btn.click(
                fn=debug_voice_store,
                outputs=[register_voice_status]
            )
            
            # Refresh dropdown khi c√≥ thay ƒë·ªïi
            lecture_voice_sample.change(
                fn=refresh_voice_dropdown,
                outputs=[lecture_voice_id]
            )

            gr.Markdown("### üìù N·ªôi dung t·ª´ PowerPoint")
            lecture_slides_preview = gr.Textbox(label="N·ªôi dung ƒë√£ tr√≠ch xu·∫•t t·ª´ c√°c slide", lines=8, interactive=False)

            extract_lecture_slides_btn = gr.Button('üìÇ Tr√≠ch xu·∫•t n·ªôi dung t·ª´ PowerPoint', variant='secondary')
            generate_lecture_btn = gr.Button('üé¨ T·∫°o Video B√†i Gi·∫£ng', variant='primary')
            lecture_status = gr.Textbox(label="Tr·∫°ng th√°i x·ª≠ l√Ω", interactive=False)

        with gr.Column(variant='panel'):
            with gr.Tabs():
                with gr.TabItem('‚öôÔ∏è C√†i ƒë·∫∑t'):
                    lecture_pose_style = gr.Slider(minimum=0, maximum=46, step=1, label="Pose style", value=0)
                    lecture_size_of_image = gr.Radio([256, 512], value=256, label='ƒê·ªô ph√¢n gi·∫£i ·∫£nh')
                    lecture_preprocess_type = gr.Radio(['crop', 'resize','full', 'extcrop', 'extfull'], value='crop')
                    lecture_is_still_mode = gr.Checkbox(label="Still Mode")
                    lecture_batch_size = gr.Slider(label="Batch size", step=1, maximum=10, value=6)
                    lecture_enhancer = gr.Checkbox(label="GFPGAN Face enhancer")
                    lecture_fast_mode_btn = gr.Button('‚ö° Ch·∫ø ƒë·ªô nhanh')
                    lecture_fast_mode_btn.click(
                        fn=set_lecture_fast_mode,
                        outputs=[lecture_size_of_image, lecture_preprocess_type, lecture_is_still_mode, lecture_batch_size, lecture_enhancer, lecture_pose_style]
                    )
            with gr.Tabs():
                gr.Markdown("### üé¨ Video B√†i Gi·∫£ng")
                lecture_final_video = gr.Video(label="Video ho√†n ch·ªânh", format="mp4")
                lecture_info = gr.Textbox(label="Th√¥ng tin chi ti·∫øt", lines=4, interactive=False)

    extract_lecture_slides_btn.click(
        fn=extract_lecture_slides,
        inputs=[lecture_pptx_file],
        outputs=[lecture_slides_preview]
    )

    # K·∫øt n·ªëi generate button v·ªõi handler
    def handle_generate_lecture(pptx_file, source_image, voice_id, preprocess_type, is_still_mode, enhancer, batch_size, size_of_image, pose_style):
        """Handler cho vi·ªác t·∫°o video b√†i gi·∫£ng"""
        try:
            if not source_image:
                return "‚ùå Vui l√≤ng ch·ªçn ·∫£nh gi√°o vi√™n", None, "‚ùå Thi·∫øu ·∫£nh gi√°o vi√™n"
            
            if not pptx_file:
                return "‚ùå Vui l√≤ng ch·ªçn file PowerPoint", None, "‚ùå Thi·∫øu file PowerPoint"
            
            if not voice_id:
                return "‚ùå Vui l√≤ng ch·ªçn gi·ªçng nh√¢n b·∫£n", None, "‚ùå Thi·∫øu gi·ªçng nh√¢n b·∫£n"
            
            # Ki·ªÉm tra voice c√≥ t·ªìn t·∫°i kh√¥ng
            if not has_voice("current_user", voice_id):
                return f"‚ùå Gi·ªçng n√≥i '{voice_id}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ƒëƒÉng k√Ω l·∫°i!", None, f"‚ùå Voice '{voice_id}' kh√¥ng t·ªìn t·∫°i"
            
            # L·∫•y text t·ª´ slides preview (c·∫ßn implement)
            slides_text = "Hello everybody, I will be teach you..."  # Placeholder
            
            # T·∫°o audio v·ªõi voice ƒë√£ ch·ªçn
            audio_path = convert_text_to_audio_with_voice(slides_text, "current_user", voice_id)
            
            if not audio_path:
                return "‚ùå Kh√¥ng th·ªÉ t·∫°o audio", None, "‚ùå L·ªói t·∫°o audio"
            
            # TODO: Implement SadTalker pipeline
            # result_video = generate_sadtalker_video(source_image, audio_path, ...)
            
            return "‚úÖ ƒêang t·∫°o video...", None, f"‚úÖ Audio ƒë√£ t·∫°o: {audio_path}"
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói: {str(e)}"
            return error_msg, None, error_msg

    # K·∫øt n·ªëi generate button
    generate_lecture_btn.click(
        fn=handle_generate_lecture,
        inputs=[
            lecture_pptx_file,
            lecture_source_image,
            lecture_voice_id,
            lecture_preprocess_type,
            lecture_is_still_mode,
            lecture_enhancer,
            lecture_batch_size,
            lecture_size_of_image,
            lecture_pose_style
        ],
        outputs=[
            lecture_status,
            lecture_final_video,
            lecture_info
        ]
    )

    return {
        'source_image': lecture_source_image,
        'pptx_file': lecture_pptx_file,
        'voice_id': lecture_voice_id,
        'slides_preview': lecture_slides_preview,
        'extract_btn': extract_lecture_slides_btn,
        'generate_btn': generate_lecture_btn,
        'status': lecture_status,
        'pose_style': lecture_pose_style,
        'size_of_image': lecture_size_of_image,
        'preprocess_type': lecture_preprocess_type,
        'is_still_mode': lecture_is_still_mode,
        'batch_size': lecture_batch_size,
        'enhancer': lecture_enhancer,
        'fast_mode_btn': lecture_fast_mode_btn,
        'final_video': lecture_final_video,
        'info': lecture_info
    }
